# Mining and Mapping the Protoweb

This project mines and maps the Protoweb, an open-source collection of restored websites from the early 2000s. The goal is to understand the structure and behavior of the early internet by analyzing a dataset of approximately 3001 pages across 40 domains. For a full understanding of the project's background and goals, please refer to the research paper "Mining and Mapping the Protoweb.pdf".

## Features

*   **Web Crawling:** Crawls websites using a proxy, with options to enforce a domain boundary.
*   **Data Storage:** Stores crawled data in CSV files and a SQLite database.
*   **Graph Creation:** Creates a Neo4j graph from the crawled data to represent the web structure.
*   **Data Analysis:** Provides basic analysis of the crawled data using SQL and pandas.

## System Architecture

The web crawler is built on a class-based system with three main components:

1.  **Initialization:** Handles setup tasks, managing crawler parameters, maintaining HTTP connections, and organizing output directories.
2.  **Data Collection:** Manages URL processing, network requests, HTML content extraction, and link discovery.
3.  **Storage:** Handles data management through CSV writing and HTML content storage.

## Database Preparation

The crawled data is stored in three different formats:

*   **CSV:** A raw data repository that captures the details regarding the URL connections.
*   **SQLite:** A relational database with `pages` and `links` tables for structured data management and analysis.
*   **Neo4j:** A graph database that represents the web crawling data as an interconnected network of nodes (URLs) and edges (links).

## Project Structure

```
.
├── final_create_neo4j_graph.py
├── final_create_sqlite.py
├── final_domain_no_domain_boundary.py
├── final_domain_with_domain_boundary.py
├── final_sqlanalysis.py
├── domains.txt
├── extra_unfinished_data/
├── final_data/
└── Mining and Mapping the Protoweb.docx
```

## Scripts

### `final_domain_with_domain_boundary.py`

*   **Purpose:** Crawls websites starting from a list of domains in `domains.txt`, staying within the same domain.
*   **Input:** `domains.txt`
*   **Output:**
    *   `crawl_results_domain_boundary.csv`: A CSV file containing the crawled data.
    *   `html_pages_domain_boundary/`: A directory containing the saved HTML files.
*   **Proxy:** `http://wayback.protoweb.org:7856`

### `final_domain_no_domain_boundary.py`

*   **Purpose:** Crawls websites starting from a list of domains in `domains.txt`, without enforcing a domain boundary.
*   **Input:** `domains.txt`
*   **Output:**
    *   `crawl_results_http_only.csv`: A CSV file containing the crawled data.
    *   `html_pages__http_only/`: A directory containing the saved HTML files.
*   **Proxy:** `http://wayback2.protoweb.org:7851`

### `final_create_sqlite.py`

*   **Purpose:** Creates a SQLite database (`web_crawler.db`) and populates it with data from `crawl_results_with_domain_boundary.csv`.
*   **Input:** `crawl_results_with_domain_boundary.csv`
*   **Output:** `web_crawler.db`

### `final_create_neo4j_graph.py`

*   **Purpose:** Loads data from a CSV file into a Neo4j graph database.
*   **Input:** `crawl_results_http_only.csv` (can be configured for `crawl_results_with_domain_boundary.csv`).
*   **Output:** A graph in the configured Neo4j database.
*   **Note:** The script contains a warning about differences in the CSV format between the two crawl variations.

### `final_sqlanalysis.py`

*   **Purpose:** Performs basic analysis on the `web_crawler.db` database.
*   **Input:** `web_crawler.db`
*   **Output:** Prints analysis results to the console.

## How to Run

1.  **Install Dependencies:**
    ```bash
    pip install requests beautifulsoup4 pandas neo4j
    ```
2.  **Configure `domains.txt`:** Add the seed domains to this file, one per line.
3.  **Run the Crawler:** Choose one of the crawler scripts to run:
    *   For domain-bounded crawling: `python final_domain_with_domain_boundary.py`
    *   For unbounded crawling: `python final_domain_no_domain_boundary.py`
4.  **Create the SQLite Database:** `python final_create_sqlite.py`
5.  **Load the Neo4j Graph:**
    *   Make sure your Neo4j database is running.
    *   Update the `NEO4J_URI`, `NEO4J_USER`, and `NEO4J_PASSWORD` constants in `final_create_neo4j_graph.py`.
    *   Run the script: `python final_create_neo4j_graph.py`
6.  **Analyze the Data:** `python final_sqlanalysis.py`

## Data

*   **`crawl_results_domain_boundary.csv` / `crawl_results_http_only.csv`:** These CSV files contain the raw crawl data.
*   **`web_crawler.db`:** A SQLite database containing the structured crawl data.
*   **`html_pages_domain_boundary/` / `html_pages__http_only/`:** These directories contain the HTML source of the crawled pages.
*   **`final_data/`:** This directory contains the final data generated by the project.
*   **`extra_unfinished_data/`:** This directory contains extra data that was not used in the final project.
